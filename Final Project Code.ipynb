{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "S-0pAUzAaKss"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Step 1: Uploading Dataset ------------------\n",
        "def setup_kaggle_api(kaggle_json=\"kaggle.json\"):\n",
        "    \"\"\"\n",
        "    Sets up the Kaggle API key for downloading datasets.\n",
        "    \"\"\"\n",
        "    kaggle_dir = os.path.expanduser(\"~/.kaggle\")\n",
        "    os.makedirs(kaggle_dir, exist_ok=True)\n",
        "    destination = os.path.join(kaggle_dir, \"kaggle.json\")\n",
        "\n",
        "    if os.path.exists(kaggle_json):\n",
        "        shutil.move(kaggle_json, destination)\n",
        "        os.chmod(destination, 0o600)\n",
        "        print(\"Kaggle API key set up successfully.\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"ERROR: kaggle.json not found!\")\n",
        "\n",
        "\n",
        "def download_and_unzip_dataset(kaggle_dataset, output_dir=\"marine_animal_dataset\"):\n",
        "    \"\"\"\n",
        "    Downloads and unzips a dataset using Kaggle API.\n",
        "    \"\"\"\n",
        "    print(\"Downloading dataset from Kaggle...\")\n",
        "    os.system(f\"kaggle datasets download -d {kaggle_dataset}\")\n",
        "    print(\"Unzipping dataset...\")\n",
        "    os.system(f\"unzip marine-animal-images.zip -d {output_dir}\")\n",
        "    print(f\"Dataset unzipped successfully to {output_dir}.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "edkZFUZnaP6Y"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Step 2: Exploring the Dataset ------------------\n",
        "\n",
        "def list_directory_contents(directory):\n",
        "    \"\"\"\n",
        "    Lists the contents of a directory.\n",
        "    \"\"\"\n",
        "    if os.path.exists(directory):\n",
        "        return os.listdir(directory)\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"ERROR: Directory '{directory}' not found!\")\n",
        "\n",
        "def display_sample_images_grid(directory, num_samples=5, grid_size=(2, 3)):\n",
        "    \"\"\"\n",
        "    Displays sample images from each class in the directory in a grid format.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the directory containing class folders.\n",
        "        num_samples (int): Number of images to display per class.\n",
        "        grid_size (tuple): Number of rows and columns in the grid.\n",
        "    \"\"\"\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "    classes = list_directory_contents(directory)\n",
        "\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            print(f\"Displaying images from class: {class_name}\")\n",
        "            images = [img for img in os.listdir(class_path) if img.lower().endswith(tuple(image_extensions))]\n",
        "\n",
        "            # Adjust the number of samples based on available images\n",
        "            num_images = min(num_samples, len(images))\n",
        "            fig, axes = plt.subplots(grid_size[0], grid_size[1], figsize=(12, 8))\n",
        "            fig.suptitle(f\"Class: {class_name}\", fontsize=16)\n",
        "\n",
        "            # Display images in the grid\n",
        "            for idx, ax in enumerate(axes.flatten()):\n",
        "                if idx < num_images:\n",
        "                    img_path = os.path.join(class_path, images[idx])\n",
        "                    image = cv2.imread(img_path)\n",
        "                    if image is not None:\n",
        "                        ax.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "                        ax.axis(\"off\")\n",
        "                        ax.set_title(f\"Image {idx + 1}\")\n",
        "                else:\n",
        "                    ax.axis(\"off\")  # Turn off extra axes\n",
        "\n",
        "            plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit title\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "WZ2Y8VXUaR2I"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Step 3: Preprocessing and EDA ------------------\n",
        "\n",
        "def preprocess_image(image_path, size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Preprocesses a single image: resize and normalize.\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(size)\n",
        "    return np.array(img) / 255.0\n",
        "\n",
        "\n",
        "def count_images_per_class(directory):\n",
        "    \"\"\"\n",
        "    Counts the number of images in each class directory.\n",
        "    \"\"\"\n",
        "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
        "    classes = list_directory_contents(directory)\n",
        "    class_counts = {}\n",
        "    for class_name in classes:\n",
        "        class_path = os.path.join(directory, class_name)\n",
        "        if os.path.isdir(class_path):\n",
        "            image_files = [\n",
        "                img for img in os.listdir(class_path) if img.lower().endswith(tuple(image_extensions))\n",
        "            ]\n",
        "            class_counts[class_name] = len(image_files)\n",
        "    return class_counts\n"
      ],
      "metadata": {
        "id": "hj92s0FZaYjy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Step 4: Model Training ------------------\n",
        "\n",
        "def build_model(input_shape=(224, 224, 3), num_classes=9):\n",
        "    \"\"\"\n",
        "    Builds and compiles a ResNet50 model for training.\n",
        "    \"\"\"\n",
        "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=input_shape)\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(128, activation=\"relu\")(x)\n",
        "    predictions = Dense(num_classes, activation=\"softmax\")(x)\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "9j9O712Fabcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Step 5: Integration with WWF Data ------------------\n",
        "\n",
        "def integrate_wwf_data(predictions, wwf_csv):\n",
        "    \"\"\"\n",
        "    Integrates predictions with WWF conservation data.\n",
        "    \"\"\"\n",
        "    # Load WWF data\n",
        "    wwf_data = pd.read_csv(wwf_csv)\n",
        "\n",
        "    # Inspect the dataset\n",
        "    print(\"Columns in the WWF dataset:\", wwf_data.columns)\n",
        "    if 'species' in wwf_data.columns:\n",
        "        wwf_data['species'] = wwf_data['species'].str.lower().str.strip()\n",
        "    elif 'Species' in wwf_data.columns:  # Adjust based on actual column name\n",
        "        wwf_data['Species'] = wwf_data['Species'].str.lower().str.strip()\n",
        "        wwf_data = wwf_data.rename(columns={'Species': 'species'})\n",
        "    else:\n",
        "        raise KeyError(\"The WWF dataset does not contain a 'species' column.\")\n",
        "\n",
        "    # Remove duplicates\n",
        "    wwf_data = wwf_data.drop_duplicates(subset='species')\n",
        "\n",
        "    # Map predictions to conservation status\n",
        "    results = pd.DataFrame({\n",
        "        'Predicted Species': predictions,\n",
        "        'Conservation Status': [\n",
        "            wwf_data[wwf_data['species'] == sp.lower().strip()]['status'].values[0]\n",
        "            if len(wwf_data[wwf_data['species'] == sp.lower().strip()]) > 0 else 'Unknown'\n",
        "            for sp in predictions\n",
        "        ]\n",
        "    })\n",
        "    print(results)\n",
        "\n"
      ],
      "metadata": {
        "id": "J-MZSZBjafTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Main Workflow ------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Step 1: Setup and download dataset\n",
        "    kaggle_dataset = \"mikoajfish99/marine-animal-images\"\n",
        "    setup_kaggle_api()\n",
        "    download_and_unzip_dataset(kaggle_dataset)\n",
        "\n",
        "    # Step 2: Explore dataset with improved image display\n",
        "    images_dir = os.path.join(\"marine_animal_dataset\", \"images\", \"train\")\n",
        "    display_sample_images_grid(images_dir, num_samples=6, grid_size=(2, 3))\n",
        "\n",
        "    # Step 3: Perform EDA\n",
        "    class_counts = count_images_per_class(images_dir)\n",
        "    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n",
        "    plt.title(\"Class Distribution\")\n",
        "    plt.xlabel(\"Classes\")\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "    # Step 4: Train the model\n",
        "    model = build_model()\n",
        "    # Additional steps for training...\n",
        "\n",
        "    # Step 5: Integrate WWF data\n",
        "    predictions = [\"leatherback turtle\", \"blue whale\", \"great white shark\"]\n",
        "    integrate_wwf_data(predictions, \"WWF list of endangered marine animals  - Sheet1.csv\")\n",
        "\n"
      ],
      "metadata": {
        "id": "LjN7z7kZairf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}